{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The world of the efficient MapReduce is based on three whales. Combiner, Partitioner, and Comparator.**\n",
    "\n",
    "# Combiner\n",
    "\n",
    "To change the usage of these IO operations and network bandwidth, you can use **combiner** to squash several items into one.\n",
    "<img src=\"./images/cb1.png\" width=300>\n",
    "\n",
    "Combiner:\n",
    "- expects an input in the form of the reducer input and it has the same output signature as a mapper. \n",
    "- can be applied arbitrarily number of times between map and reduce phases.\n",
    "- A combiner should not change a type and format of a key and value \n",
    "\n",
    "<img src=\"./images/cb2.png\" width=350>\n",
    "\n",
    "In the word count application, there is no difference between the combiner and the reducer. So you can easily call it with the following arguments. \n",
    "\n",
    "<img src=\"./images/cb3.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Mean Value\n",
    "\n",
    "> count how many times on average you see a word in an article\n",
    "\n",
    "- Mapper.py: print a pair containing the number of articles processed and the cumulative amount of words\n",
    "\n",
    "<img src=\"./images/cb4.png\" width=300>\n",
    "\n",
    "- Reducer: memorize not only the number of occurrences but also the number of articles.\n",
    "\n",
    "<img src=\"./images/cb5.png\" width=300>\n",
    "\n",
    "- Combiner: It could help us to speed up calculations, for the whole MapReduce job, as you will use less IO resources\n",
    "<img src=\"./images/cb61.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Median\n",
    "\n",
    "you have to get the whole dataset in place. So, the combiner is out of help in this case.\n",
    "\n",
    "it's not always possible to speed up calculations with the combiner.\n",
    "<img src=\"./images/cb7.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioner\n",
    "\n",
    "Partitioner is used to calculate a reducer index for each (key, value) pair\n",
    "\n",
    "\n",
    "Collocation 搭配字詞\n",
    "\n",
    "> To find collocations of size two in a data sets, you need to count Bigrams. \n",
    "\n",
    "**Mapper:**\n",
    "The following mapper will emit a sequence of bigrams followed aggregation during their use phase. \n",
    "\n",
    "<img src=\"./images/pt1.png\" width=300>\n",
    "\n",
    "Hadoop MapReduce frame work will distribute and sort data by the first word. Because everything before the first tab character is considered a key. \n",
    "\n",
    "**Reducer:**\n",
    "To sort data by the second word, you can update reducer.py to count all bigrams for the first corresponding word in memory-->memory consuming.\n",
    "<img src=\"./images/pt2.png\" width=300>\n",
    "\n",
    "output of these MapReduce application which validates that New York bigram is a collocation\n",
    "<img src=\"./images/pt3.png\" width=300>\n",
    "\n",
    "\n",
    "In addition to the unnecessary memory consumption there would be uneven lot on the reducers. \n",
    "\n",
    "The benefit of MapReduce: it provides functionality to **parallelized work.**\n",
    "\n",
    "e.g.  In a default scenario you will have the far more lot on the reducer that will be busy processing this article ```The```. But you have no need to send all of the bigrams starting with ```The``` to one reducer as you do calculations for each pair of words independently.\n",
    "<img src=\"./images/pt4.png\" width=350>\n",
    "\n",
    "** partitioner**:\n",
    "> In this case you would like to split the line into key value pairs by the second tab character.\n",
    "\n",
    "<img src=\"./images/pt5.png\" width=350>\n",
    "- complete this MapReduce job faster due to better parallelism.\n",
    "- bigrams starting with any arbitrary word allocated in different files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: IPv4 network addresses\n",
    "\n",
    "IPv4 address contains four numbers called Octets delimited by dots.\n",
    "\n",
    "<img src=\"./images/pt6.png\" width=350>\n",
    "\n",
    "You can specify what a delimiter is and set number of fields related to a key. MapReduce framework will substitute this particular delimiter between num and num+1 fields to a tab character without any changes in your streaming scripts.\n",
    "\n",
    "In this example,\n",
    "- I would like to split the output from the streaming mapper by the first dot.\n",
    "- And from the reducers stream and output, I substituted the next but one dot with a key value MapReduce delimiter, which is a tab character.\n",
    "\n",
    "<img src=\"./images/pt7.png\" width=350>\n",
    "\n",
    "> To partition IPv4 addresses by the second character of a first octet\n",
    "\n",
    "- specify the field index and the starting character index in the start position\n",
    "- specify the field index and the character index in the end position.\n",
    "\n",
    "<img src=\"./images/pt8.png\" width=350>\n",
    "\n",
    "I have to set a special partitioner called KeyFieldBasedPartitioner.\n",
    "\n",
    "*Bigger picture*:  the whole pipeline of MapReduce application execution\n",
    "\n",
    "<img src=\"./images/pt9.png\" width=400>\n",
    "\n",
    "You have mappers at the top. Then the data goes through combiners, then it is distributed by the partitioner. Finally there is a reduced phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparator \n",
    "\n",
    "All the keys in MapReduce implement writable comparable interface.\n",
    "\n",
    "**Comparable** means that you can specify the rule according to which one key is bigger than another.\n",
    "\n",
    "By default, you have the keys sorted by increasing order. For some applications, you would like to store them in a reverse order. \n",
    "\n",
    "comparator compares records for sorting, it is not an optimization\n",
    "\n",
    "## Example: IPv4 network addresses\n",
    "\n",
    "> To sort octets of IPV4 address by the second octet in an increasing order, and by the third octate in a reverse order.\n",
    "\n",
    "<img src=\"./images/cp1.png\" width=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speculative Execution / Backup Tasks\n",
    "\n",
    "\n",
    "> reduce your total waiting time by a factor of two\n",
    "\n",
    "One of the most common problems that causes a MapReduce application to wait longer for a job completion is a **straggler**--a machine that takes an unusually long time to complete one of the last few tasks in the computation.\n",
    "<img src=\"./images/se2.png\" width=450>\n",
    "\n",
    "The solution of straggler: **Backup Tasks**\n",
    "\n",
    "<img src=\"./images/se1.png\" width=450>\n",
    "Due to the deterministic behavior of the Mapper and Reducer, you can easily re-execute straggler body of work on other node. \n",
    "\n",
    "In this case, the worker which processes data, they first outputs data to a distributed file system. All the other concurrent executions will be killed. \n",
    "\n",
    "Of course, the MapReduce framework is not going to have a copy for each running task. It is only used when a MapReducer application is close to completion. \n",
    "\n",
    "\n",
    "\n",
    "## Tuning:\n",
    "<img src=\"./images/se3.png\" width=450>\n",
    "- Speculative Execution is set by default to true. set these flags to false if you don't allow multiple instances of some map or reduce task to be executed in parallel.\n",
    "\n",
    "<img src=\"./images/se4.png\" width=450>\n",
    "- These two flags can be used to specify the allowed number of running backup tasks at each point in the stream of the time and overall. \n",
    "\n",
    "<img src=\"./images/se5.png\" width=450>\n",
    "- you can tune timeouts in milliseconds that will limit the time of your waiting till the next round of speculation.\n",
    "\n",
    "\n",
    "If you have successfully managed to speed up the process with speculation, then you should be able to find concurrent tasks killed by speculation on job trigger. \n",
    "<img src=\"./images/se7.png\" width=550>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression\n",
    "\n",
    "You can balance the process and capacity by the data **compression**.\n",
    "\n",
    "Data compression is essentially a trade-off between\n",
    "- the disk I/O required to read and write data\n",
    "- The network bandwidth required to send data across the network\n",
    "- the in-memory calculation capacity(speed and usage of CPU and RAM). \n",
    "- ability of archives to be splitted by Hadoop\n",
    "\n",
    "The correct balance of these factors depends on the characteristics of your cluster, your data, your applications, or usage patterns, and the weather forecast.\n",
    "\n",
    ">Data located in HDFS can be compressed. \n",
    "There is a shuffle and sort phase between map and the reduce where you can compress the *intermediate data.*-->optimization\n",
    "\n",
    "<img src=\"./images/comp1.png\" width=350>\n",
    "\n",
    "- Splittable column means that you can cut a file at any place and find the location for the next or the previous valid record.\n",
    "- Native libraries that provide implementation of compression and decompression functionality, usually also support an option to choose a trade-off between speed or space optimization.\n",
    "\n",
    "**Pros & Cons:**\n",
    "- gzip file is a deflate file with extra headers and a footer. \n",
    "- bzip is more aggressive for space requirements, but consequently, it's slower during the compression. \n",
    "- lzo files can be used where you read data far more frequently than write.\n",
    "- You can provide index files for lzo files to make them splittable. \n",
    "- Snappy,even more faster decompression, but you will only be able to split this file records.\n",
    "\n",
    "A **Hadoop codec** is an implementation of a compression, decompression algorithm.\n",
    "\n",
    "<img src=\"./images/comp2.png\" width=350>\n",
    "\n",
    "You can specify the compression parameters for intermediate data for output or for both\n",
    "\n",
    "<img src=\"./images/comp3.png\" width=350>\n",
    "\n",
    "**Rules of thumb**:\n",
    "1. *gzip* or *bzip* are a good choice for **cold data**, which is accessed infrequently.\n",
    "2. *bzip* produce more compression than *gzip* for some kinds of files at the cost of some speed when compressing and decompressing. \n",
    "3. *Snappy* or *lzo* are a better choice for **hot data**, which is accessed frequently. \n",
    "4. *Snappy* often performs better than *lzo*.\n",
    "5. For MapReduce, we can use bzip and lzo formats, if you would like to have your data splittable.\n",
    "6. *Snappy* and *gzip* formats are not splittable at file level compression. But you can use block level compression and splittable container formats such as *Avro* or *SequenceFile*--> process the blocks in parallel using MapReduce\n",
    "\n",
    "<img src=\"./images/comp4.png\" width=350>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unreliable Components\n",
    "3 different unreliable components in distributed systemsnodes: nodes, links and clock.\n",
    "Distributed computational systems:\n",
    "- built from unreliable components\n",
    "- Cluster nodes can break any time because of power supply, disk damages, overheated CPUs, and so on\n",
    "\n",
    "3 different unreliable components in distributed systemsnodes: nodes, links and clock.\n",
    "\n",
    "## Nodes\n",
    "**3 types of node failures:**\n",
    "\n",
    "- **Fail-Stop**: if machines get out of service during a computation then you have to have an external impact to bring system back to a working state.\n",
    "> A system administrator should either \n",
    " - fix the node and reboot the whole system or part of it. \n",
    " Or\n",
    " <img src=\"./images/week2_0.png\" width=200>\n",
    " - retire the broken machine and reconfigure the distributed system\n",
    " \n",
    "<img src=\"./images/week2_1.png\" width=200>\n",
    "\n",
    "\n",
    "- **Fail-Recovery**: during computations, notes can arbitrarily crash and return back to servers. \n",
    " - doesn't influence correctness and success of computations\n",
    " - no external impact necessary to reconfiguring the system at such events.\n",
    "> if a hard drive was damaged, then a system administrator can physically change the hard drive.After reconnection, this node will be automatically picked up by a distributed system. And it will even be able to participate in current computations.\n",
    "\n",
    "<img src=\"./images/week2_2.png\" width=200>\n",
    "\n",
    "- **Byzantine**:\n",
    "A distributed system is robust Byzantine failures if it can correctly work despite some of the nodes behaving out of protocol.\n",
    "<img src=\"./images/week2_3.png\" width=200>\n",
    "\n",
    "> If you are developing a financial system, then you are likely required to deal with these types of failures to protect your customers and your business.\n",
    "\n",
    "## Links\n",
    "**3 types of links:**\n",
    "\n",
    "- perfect: all the sent messages must be delivered and received without any modification into the same portal.\n",
    "<img src=\"./images/week2_4.png\" width=200>\n",
    "\n",
    "- fail-loss: some part of the messages can be lost but the probability of message loss does not depend on contents of a message.\n",
    "\n",
    "> the well-known TCP/IP protocol tries to solve this problem by re-transmitting messages if they were not received.\n",
    "\n",
    "\n",
    "<img src=\"./images/week2_5.png\" width=350>\n",
    "- byzantine: some messages can be filtered according to some rule, some messages can be modified, and some messages could be created out of nowhere\n",
    "\n",
    "## Clocks\n",
    "\n",
    "**clock synchronization problem: **\n",
    "\n",
    "- clock skew: the time can be different on different machines\n",
    "- clock drift: there can be a different clock rate\n",
    "<img src=\"./images/week2_6.png\" width=300>\n",
    "\n",
    "**Logical clocks** help to track happened before events and therefore, order events to build reliable protocols. \n",
    "\n",
    "## [A]synchronous systems\n",
    "Systems can be divided into synchronous and asynchronous.\n",
    "\n",
    "**synchronous**: \n",
    "- every network packet should be delivered within a limited time bound. \n",
    "- Clock drift is limited in size\n",
    "- each CPU instruction is also limited in time.\n",
    "\n",
    "### Examples of different distributed systems\n",
    "\n",
    "1. fail-stop, perfect link, and synchronous model\n",
    "\n",
    "> A parallel computational model and widely adopted by supercomputers where many processors connected by a local high speed computer bus. \n",
    "\n",
    "2. fail-recovery, fair-loss link, and asynchronous model (this course focus)\n",
    "\n",
    "3. byzantine-failure, byzantine link, and asynchronous model\n",
    "\n",
    "> computational components spread across the globe of unreliable and untrusted network connections. The common representative of this model is grid computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two phases during computation, map and reduce:\n",
    "\n",
    "**map**:apply the same function to each element of your collection.\n",
    "<img src=\"./images/week2_8.png\" width=200>\n",
    "\n",
    "**reduce**:Reduce operator causes a sequence of elements by applying the following procedure iteratively.\n",
    "- As soon as you have more than one element in the sequence, then you get the first two and combine them to one element by applying the provided function.\n",
    "- Reduce function computes the value from left to right.\n",
    ">Be careful about reducing functions that are not associative.\n",
    "\n",
    "<img src=\"./images/week2_9.png\" width=200>\n",
    "\n",
    "**MapReduce**:the class of problems that you can solve with arbitrary map and reduce functions is quite big.\n",
    "<img src=\"./images/week2_10.png\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Shell\n",
    "\n",
    "- run a distributed **grep** as a MapReduce job.\n",
    " <img src=\"./images/week2_12.png\" width=270>\n",
    " - Map will be equivalent to grep\n",
    " - Reduce will be None. \n",
    "\n",
    "In MapReduce applications, you don't always need map or reduce function. \n",
    "\n",
    "- run a distributed **head** as a MapReduce job.\n",
    " <img src=\"./images/week2_13.png\" width=270>\n",
    "You can just retrieve the necessary data with HDFS client. To get these data with MapReduce job, get actual information such as, block index and size in lines on map phase to complete the task correctly --> **head and tweaks**.\n",
    "\n",
    "- run a distributed **wc** as a MapReduce job.\n",
    " <img src=\"./images/week2_14.png\" width=270>\n",
    " - The output from the map will be a tuple of the size 3: number of lines, words, and bytes.\n",
    " - Sum the items by components, so the reduced function will be an extended add operator.\n",
    "\n",
    "\n",
    "## WordCound Example\n",
    "\n",
    "*find the most popular words in the Wikipedia with MapReduce*\n",
    "\n",
    " 1. count how many times each word appear in a data set\n",
    "> wikipedia.dump | tr ' ' '\\n' | sort | uniq -c\n",
    "\n",
    "Map-->Shuffle&Sort-->Reduce\n",
    "- Map: the text is split in two words - tr\n",
    "- Shuffle&Sort: words are distributed to a reduce phase in a way that reduce functions can be executed independently on different machines - sort\n",
    "- Reduce: uniq\n",
    "\n",
    " <img src=\"./images/mr2.png\" width=370>\n",
    " **external sorting**:If the data is sorted, and can be read as a stream, then uniq-c will be working correctly. To make data sorted, you only need to have enough disk space. The algorithm for this is called, external sorting.\n",
    " \n",
    "All input and output of map and reduce functions should be a key value pair.\n",
    "\n",
    "**MapReduce Formal Model**:\n",
    "\n",
    "map: (key, value) → (key, value)\n",
    "\n",
    "reduce: (key, value) → (key, value)\n",
    "\n",
    "```bash\n",
    "$ cat -n wikipedia.dump | tr ' ' '\\n'| sort | uniq -c\n",
    "```\n",
    "\n",
    "\n",
    "- read data and get pairs with a line number, and line content.\n",
    "\n",
    "> cat -n wikipedia.dump: [(line_no, line), …]\n",
    "\n",
    "- on a map phase,you ignore line numbers and split lines into words. \n",
    "> tr ' ' '\\n': (-, line) —> [ (word, 1), … ]\n",
    "\n",
    " - You can add value one to each output it worked. So, it means that you have seen this word once by reading a line from left to right. \n",
    "\n",
    "- a shuffle and sort phase where you spread the words by the hashes.So, you can process them on independent reducers. \n",
    "> sort: Shuffle & Sort\n",
    "\n",
    "- count how many figures of 1 you have for each word, and sum them up to get an answer. \n",
    "> uniq -c: (word, [1, …]) —> (word, count)\n",
    "\n",
    "\n",
    "<img src=\"./images/mr4.png\" width=370>\n",
    "\n",
    "**3 types of key value pairs**:\n",
    "- Key value pairs for the input data\n",
    "```bash\n",
    "cat -n wikipedia.dump: [(line_no, line), …]\n",
    "read: [(k_in, v_in), …]\n",
    "```\n",
    "- key value pairs for the intermediate data\n",
    "```bash\n",
    "tr ‘ ‘ ‘\\n’: (-, line) —> [ (word, 1), … ]\n",
    "map: (k_in, v_in) —> [(k_interm, v_interm), …]\n",
    "```\n",
    "- key value pairs for the output data\n",
    "```bash\n",
    "Shuffle & Sort: sort and group by k_interm\n",
    "uniq -c: (word, [1, …]) —> (word, count)\n",
    "reduce: (k_interm, [(v_interm, …)] ) —> [(k_out, v_out), …]\n",
    "```\n",
    "<img src=\"./images/mr6.png\" width=370>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault Tolerence\n",
    "In a distributed file system, you store information with duplication in order to overcome node failures. MapReduce framework should also provide robustness against node failures during the job execution.\n",
    "\n",
    "In a MapReduce job, you will have:\n",
    "1. Master program: control the execution\n",
    "<img src=\"./images/ft1.png\" width=370>\n",
    "\n",
    "2. Master program will launch mappers to process input blocks or splits of data. \n",
    "<img src=\"./images/ft2.png\" width=370>\n",
    "\n",
    "3. To overcome the issues of correction execution mappers, there is no harm in re-executing mapper against the same data because you expect map function to be deterministic. As soon as you work on top of HDFS, you have a replica of this data on other nodes. So, you could assign another worker to a execute mapper against these data, and application master will do all this magic for you.\n",
    "<img src=\"./images/ft3.png\" width=370>\n",
    "\n",
    "4. If a worker running reducer dies, you can shuffle and sort data for this particular reducer to another worker. \n",
    "<img src=\"./images/ft4.png\" width=370>\n",
    "\n",
    "5.  Shuffled and sorted data are stored on local disks instead of the distributed file system\n",
    "<img src=\"./images/ft5.png\" width=370>\n",
    "\n",
    "> *You only need to provide deterministic map and reduce function*\n",
    "\n",
    "\n",
    "## Hadoop MapReduce framework\n",
    "\n",
    "\n",
    "**Job**\n",
    "One MapReduce application is a job.\n",
    "\n",
    "**Task**\n",
    "a task can be either mapper or reducer.\n",
    "\n",
    "### First version\n",
    "<img src=\"./images/ft7.png\" width=370>\n",
    "\n",
    "- JobTracker: one global JobTracker to direct execution of MapReduce jobs.\n",
    " - located on one high-cost and high-performance node with HDFS namenode.\n",
    " - a single point failure\n",
    "- TaskTrackers\n",
    " - located once per every node where you store data, or where datanode daemon is working\n",
    " - spawns workers from mapper or reducer\n",
    " \n",
    "### YARN\n",
    "> Yet Another Resource Negotiation\n",
    "\n",
    "<img src=\"./images/ft8.png\" width=370>\n",
    "\n",
    "- TaskTracker is subtituted by NodeManagers who can provide a layer of CPU and RAM containers.\n",
    "-  ResourceManager overseas NodeManagers, and client request resources for execution\n",
    " - MapReduce applications can work on top of this resource layer.\n",
    "- There is no concept such as a global JobTracker because application master can start on any node. \n",
    "-  All of these enable Hadoop to share resources dynamically between MapReduce and other parallel processing frameworks. \n",
    "\n",
    "<img src=\"./images/mrf.png\" width=370>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

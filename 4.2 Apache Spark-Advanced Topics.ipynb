{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution & Scheduling \n",
    "\n",
    "**SparkContext**\n",
    "- When creating a Spark application, the first thing you do is create a SparkContext object, which tells Sparks how to access a cluster.\n",
    "- The context, living in your driver program, coordinates sets of processes on the cluster to run your application.\n",
    "\n",
    "<img src=\"./images/exe1.png\" width=400>\n",
    "\n",
    "- The SparkContext object communicates the Cluster Manager to allocate executors.\n",
    "- The Cluster Manager is an external service for acquiring resources on a cluster. For example, YARN, Mesos or a standalone Spark cluster.\n",
    "- once the context has allocated the executors, it communicates directly with them and schedules tasks to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jobs, stages, tasks\n",
    "- **Task** is a unit of work to be done\n",
    "- **Tasks** are created by a **job scheduler** during the scheduling of a job for every job stage. And every task belongs to the job stage.\n",
    "- **Job** is spawned in response to a Spark action\n",
    "- **Job** is divided in smaller sets of tasks called **stages**\n",
    "\n",
    "\n",
    "### Example\n",
    "Z = X\n",
    " .map(lambda x: (x % 10, x / 10))\n",
    " .reduceByKey(lambda x, y: x + y)\n",
    " .collect()\n",
    " \n",
    "1. Whenever you invoke an action, the job gets spawned in the driver program. \n",
    "<img src=\"./images/exe2.png\" width=400>\n",
    "\n",
    "2. Then the driver runs a job scheduler to divide the job into smaller stages.\n",
    "<img src=\"./images/exe3.png\" width=400>\n",
    "\n",
    "3. Then tasks are created for every job stage.\n",
    "4. tasks are delegated to the executors, which perform the actual work.\n",
    "<img src=\"./images/exe4.png\" width=400>\n",
    " \n",
    "```\n",
    "bash\n",
    "All this machinery exists within the SparkContext object. It keeps track of the executors, it spawns jobs, and it runs the scheduler.\n",
    "```\n",
    "\n",
    "**Difference between job stage and task:**\n",
    "- **Job stage** is a pipelined computation spanning between materialization boundaries\n",
    " -  job stages are defined on RDD level, thus not immediately executable\n",
    "- **Task** is a job stage bound to particular partitions\n",
    " - bound to a particular partitions, thus immediately executable\n",
    "- **Materialization** happens when reading, shuffling or passing data to an action\n",
    " - narrow dependencies allow pipelining\n",
    " - wide dependencies forbid it\n",
    " \n",
    "**SparkContext â€“ other functions**:\n",
    "- Tracks liveness of the executors by sending heartbeat messages periodically.\n",
    " - required to provide fault-tolerance\n",
    "- Schedules multiple concurrent jobs\n",
    " - to control the resource allocation within the application\n",
    "- Performs dynamic resource allocation if the cluster manager permits.\n",
    " - increases cluster utilization in shared environments by proper scheduling of multiple applications according to their resource demands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. The SparkContext is the core of your application\n",
    " - allows your application to connect to a cluster and allocate resources and executors. \n",
    " - whenever you invoke an action, the SparkContext spawns a job and runs the job scheduler to divide it into stages-->**pipelineable**\n",
    " - tasks are created for every job stage and scheduled to the executors.\n",
    "2. The driver communicates directly with the executors\n",
    "3. Execution goes as follows:\n",
    "```bash\n",
    "Action -> Job -> Job Stages -> Tasks\n",
    "```\n",
    " \n",
    "4. Transformations with narrow dependencies allow pipelining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching & Persistence \n",
    "\n",
    "- RDDs are partitioned\n",
    "- Execution is build around the partitions\n",
    "- Each task processes a small number of partitions at a time, and the shuffle globally redistributes data items between the partitions, when required.\n",
    "- Spark transfers data over the network and the IO unit here is not a partition but a block.\n",
    "- Block is a unit of input and output in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "> Motivating example: load a wikipedia dump from HDFS and see how many articles there contain the words Spark\n",
    "\n",
    "You need to create the RDD, apply the filter transformation, and invoke the count action.\n",
    "<img src=\"./images/exe5.png\" width=400>\n",
    "\n",
    "> Motivating example: among those articles with the Spark word, you would like to see how many of them contain the word, Hadoop and how many the word MapReduce.\n",
    "\n",
    "<img src=\"./images/exe6.png\" width=400>\n",
    "\n",
    "***Perform worse!***\n",
    "\n",
    "- Reason: after completing the computation, Spark disposes intermediate data and those intermediate RDDs. That means, it will reload the Wikipedia dump two more times incurring extra input and output operations.\n",
    "\n",
    "- A better strategy:\n",
    "\n",
    "```\n",
    "cache the preloaded dump in the memory and reuse it until you end your session.\n",
    "```\n",
    "\n",
    "Spark allows you to hint which RDDs are better to be kept in memory or even on the disk. Spark does so by ***caching the blocks comprising your dataset. ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling persistence level\n",
    "\n",
    "```cache```: mark the data set as cached by invoking a cache method on it\n",
    " - The cache method is just a shortcut for the memory-only persistence. \n",
    " \n",
    "```persist```: allows you to set RDDs storage to persist across operations after the first time it is computed.\n",
    " - parameterized by a storage level\n",
    "<img src=\"./images/exe7.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best practices\n",
    "\n",
    "When running an **interactive shell**, cache your dataset after you've done all the necessary preprocessing.\n",
    "- by keeping your work inside in the memory, you would get a more responsive experience. \n",
    "\n",
    "When running a **batch computation**, cache dictionaries that you join with your data. \n",
    "- Join dictionaries are often reshuffled, so it would be helpful to speed up their read times. \n",
    "\n",
    "When running an **iterative computation**, cache static data like dictionaries or input datasets\n",
    "- avoid reloading the data from the ground up on every iteration.\n",
    "- The static data tends to get evicted due to the memory pressure from the intermediate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Performance may be improved by persisting data across operations\n",
    " - in interactive sessions, iterative computations and hot datasets\n",
    "- You can control the persistence of a dataset\n",
    " - whether to store in the memory or on the disk\n",
    " - how many replicas to create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcast Variable \n",
    "**shared memory** is a powerful abstraction, but often misused.\n",
    "- It can make the developer's life easier\n",
    "- It can make the application performance deteriorate because of extra synchronization.\n",
    "\n",
    "```bash\n",
    "This is why in spark there are restricted forms of the shared memory. \n",
    "```\n",
    "\n",
    "\n",
    "**Broadcast variable** is a read-only variable that is efficiently shared among tasks\n",
    "\n",
    "**one to many communication:** When it captures a variable into the closure, it is sent to an executor together with a task specification.\n",
    "\n",
    "**many to many communication protocol**: torrent\n",
    "\n",
    "-  Distribution is done by a torrent-like protocol (extremely fast!)\n",
    "- Distributed efficiently compared to captured variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "> Motivating example: resolve IP addresses to countries from 1 terabyte access log for your website\n",
    "\n",
    "Idea: map-side join--distribute the database to every mapper and query it locally.\n",
    "\n",
    "Distributing the database via a broadcast variable, we take slightly more than 1 gigabyte of outgoing traffic at the driver node\n",
    "<img src=\"./images/br1.png\" width=400>\n",
    "\n",
    "> Motivating example: \n",
    "\n",
    "1. setup a transformation graph to compute a dictionary\n",
    "2. invoke the ```collect``` action to load it into the driver's memory\n",
    "3.  put it into the broadcast variable to use in further computations.\n",
    "\n",
    "Idea:  upload computations to spark executors and use the driver program as the coordinator.\n",
    "<img src=\"./images/br2.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Broadcast variables are read-only shared variables with effective sharing mechanism\n",
    "- Useful to share dictionaries, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulator Variable \n",
    "\n",
    "**Accumulator variable** is a read-write variable that is shared among tasks\n",
    "- Writes are restricted to increments!\n",
    " - i. e.: var += delta\n",
    " - addition may be replaced by any associate, commutative operation\n",
    " \n",
    "```bash\n",
    "Restricting the right operations allows the framework to avoid complex synchronization thus making the accumulators efficient. \n",
    "```\n",
    "- Accumulator variable could be read only by the ***driver*** program and not by the executors.\n",
    " - cannot read the accumulated value from within a task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "<img src=\"./images/acc1.png\" width=400>\n",
    "<img src=\"./images/acc2.png\" width=200>\n",
    "<img src=\"./images/acc3.png\" width=200>\n",
    "<img src=\"./images/acc4.png\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guarantees on the updates\n",
    "- Updates generated in actions: guaranteed to be applied only once to the accumulator. \n",
    " - This is because successful actions are never re-executed and Spark can conditionally apply the update.\n",
    "- Updates generated in transformations: no guarantees when they accumulate updates.  -   - Transformations can be recomputed on a failure, on the memory pressure, or in another unspecified codes like a preemption. \n",
    " - Spark provides no guarantees on how many times transformation code maybe re-executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cases\n",
    "1. Performance counters\n",
    " - number of processed records, total elapsed time, total error and so on and so forth\n",
    "2. Simple control flow\n",
    " - conditionals: stop on reaching a threshold for corrupted records\n",
    " - loops: decide whether to run the next iteration of an algorithm or not\n",
    "3. Monitoring\n",
    " - export values to the monitoring system\n",
    "4. Profiling & debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Accumulators are shared read-write variables with de-coupled read and write sides\n",
    " - could be updated from actions and transformations by using an increment. \n",
    " - can use custom associative, commutative operation for the updates\n",
    " - can read the total value only in the driver\n",
    "- Useful for the control flow, monitoring, profiling & debugging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
